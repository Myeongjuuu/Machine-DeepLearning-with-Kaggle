{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>6222902</td>\n",
       "      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>8549745</td>\n",
       "      <td>평점이 너무 낮아서...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>9311800</td>\n",
       "      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>2376369</td>\n",
       "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>9619869</td>\n",
       "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
       "...          ...                                                ...    ...\n",
       "149995   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n",
       "149996   8549745                                      평점이 너무 낮아서...      1\n",
       "149997   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
       "149998   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
       "149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
       "\n",
       "[150000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('ratingsTrain.txt', header=0, delimiter='\\t', quoting=3)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "별점10점가자 1\n"
     ]
    }
   ],
   "source": [
    "print(train_data['document'][123], train_data['label'][123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘은', '정말', '좋은', '날씨네요']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\windows\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk 데이터 다운로드\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 사용자 정의 불용어 리스트\n",
    "stop_words = set(['은', '는', '이', '가', '하', '아', '것', '들', '의', '있', '되', '수', '보', '주', '등', '한'])\n",
    "\n",
    "def preprocessing(review, stop_words=None, remove_stopwords=False):\n",
    "    if stop_words is None:\n",
    "        stop_words = set()\n",
    "\n",
    "    try:\n",
    "        # 문장에서 한글만 뽑아냄\n",
    "        review_text = re.sub(\"[^가-힣ㄱ-하-ㅣ\\\\s]\", \"\", review)\n",
    "\n",
    "        # nltk 라이브러리로 단어 단위로 분리\n",
    "        word_review = word_tokenize(review_text)\n",
    "\n",
    "        # 감탄사와 조사 제거\n",
    "        if remove_stopwords:\n",
    "            word_review = [token for token in word_review if token not in stop_words]\n",
    "\n",
    "        return word_review\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during preprocessing: {e}\")\n",
    "        return None\n",
    "\n",
    "# 예시 사용\n",
    "review = \"오늘은 정말 좋은 날씨네요!\"\n",
    "preprocessed_review = preprocessing(review, stop_words=stop_words, remove_stopwords=True)\n",
    "\n",
    "if preprocessed_review is not None:\n",
    "    print(preprocessed_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\windows\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4219/4219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 134ms/step - accuracy: 0.7194 - loss: 0.5286\n",
      "Epoch 2/5\n",
      "\u001b[1m4219/4219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 135ms/step - accuracy: 0.9467 - loss: 0.1655\n",
      "Epoch 3/5\n",
      "\u001b[1m4219/4219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 129ms/step - accuracy: 0.9839 - loss: 0.0458\n",
      "Epoch 4/5\n",
      "\u001b[1m4219/4219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 130ms/step - accuracy: 0.9890 - loss: 0.0272\n",
      "Epoch 5/5\n",
      "\u001b[1m4219/4219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 131ms/step - accuracy: 0.9909 - loss: 0.0211\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7701 - loss: 0.7586\n",
      "Evaluation - Loss: 0.7625948786735535, Accuracy: 0.7660666704177856\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk 데이터 다운로드\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 사용자 정의 불용어 리스트\n",
    "stop_words = set(['은', '는', '이', '가', '하', '아', '것', '들', '의', '있', '되', '수', '보', '주', '등', '한'])\n",
    "\n",
    "def preprocessing(review, stop_words=None, remove_stopwords=False):\n",
    "    if stop_words is None:\n",
    "        stop_words = set()\n",
    "\n",
    "    try:\n",
    "        # 문장에서 한글만 뽑아냄\n",
    "        review_text = re.sub(\"[^가-힣ㄱ-하-ㅣ\\\\s]\", \"\", review)\n",
    "\n",
    "        # nltk 라이브러리로 단어 단위로 분리\n",
    "        word_review = word_tokenize(review_text)\n",
    "\n",
    "        # 감탄사와 조사 제거\n",
    "        if remove_stopwords:\n",
    "            word_review = [token for token in word_review if token not in stop_words]\n",
    "\n",
    "        return word_review\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during preprocessing: {e}\")\n",
    "        return []\n",
    "\n",
    "# 데이터 로딩\n",
    "train_data = pd.read_csv('ratingsTrain.txt', header=0, delimiter='\\t', quoting=3)\n",
    "\n",
    "# 이상한 데이터 정제\n",
    "clean_train_review = []\n",
    "for review in train_data['document']:\n",
    "    if isinstance(review, str):\n",
    "        clean_train_review.append(preprocessing(review, stop_words=stop_words, remove_stopwords=True))\n",
    "    else:\n",
    "        clean_train_review.append([])\n",
    "\n",
    "# Text to Number\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_review)\n",
    "train_sequence = tokenizer.texts_to_sequences(clean_train_review)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 8\n",
    "train_inputs = pad_sequences(train_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "# train label 만들기\n",
    "train_labels = np.array(train_data['label'])\n",
    "\n",
    "# 10%는 Eval Data로 사용\n",
    "TEST_SPLIT = 0.1\n",
    "RNG_SEED = 13371447\n",
    "\n",
    "# 데이터를 train과 eval로 분리\n",
    "input_train, input_eval, label_train, label_eval = train_test_split(train_inputs, train_labels, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential()\n",
    "word_size = len(tokenizer.word_index) + 1\n",
    "model.add(Embedding(word_size, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(input_train, label_train, epochs=5)\n",
    "\n",
    "# 모델 평가\n",
    "loss, accuracy = model.evaluate(input_eval, label_eval)\n",
    "print(f\"Evaluation - Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "# 예측 함수\n",
    "def predict_review(sentence, model):\n",
    "    # 테스트 문장을 전처리\n",
    "    test_prepro = preprocessing(sentence, stop_words=stop_words, remove_stopwords=True)\n",
    "    test_review = []\n",
    "    test_review.append(test_prepro)\n",
    "    \n",
    "    # 전처리된 문장을 토큰화\n",
    "    test_token = tokenizer.texts_to_sequences(test_review)\n",
    "    test_seq = pad_sequences(test_token, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    ret = model.predict(test_seq)\n",
    "    \n",
    "    if ret > 0.5:\n",
    "        sentiment = \"긍정\"\n",
    "    elif ret < 0.5:\n",
    "        sentiment = \"부정\"\n",
    "    else:\n",
    "        sentiment = \"중립\"\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "긍정\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "print(predict_review('별점10점가자', model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "부정\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "print(predict_review('별로다', model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
